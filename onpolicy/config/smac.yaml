3s5z:
  # training
  n_training_threads: 16

  # algo-ppo
  num_mini_batch: 1
  gain: 0.01
  adv: 'gae_trace'
  gae_lambda: 0.95

  # network
  use_recurrent_policy: True
  stacked_frames: 1
  layer_after_N: 1
  layer_N: 2

  # seq
  share_policy: False
  use_cum_sequence: False
  use_agent_block: False
  use_sequential: True

  # Co-PPO / Sequential
  seq_strategy: 'semi_greedy'
  clip_others: True
  clip_param_tuner: True

  # lora
  train_w0: 0
  train_lora: 1
  train_lora_bias: 0

MMM2:
  # training
  n_training_threads: 16

  # algo-ppo
  num_mini_batch: 1
  gain: 0.01
  adv: 'gae_trace'
  gae_lambda: 0.95

  # network
  use_recurrent_policy: True
  stacked_frames: 1
  layer_after_N: 1
  layer_N: 2

  # seq
  share_policy: False
  use_cum_sequence: False
  use_agent_block: False
  use_sequential: True

  # Co-PPO / Sequential
  seq_strategy: 'semi_greedy'
  clip_others: True
  clip_param_tuner: True

  # lora
  train_w0: 0
  train_lora: 1
  train_lora_bias: 0


1c3s5z:
  # training
  n_training_threads: 16

  # algo-ppo
  num_mini_batch: 1
  gain: 0.01
  adv: 'gae_trace'
  gae_lambda: 0.95

  # network
  use_recurrent_policy: True
  stacked_frames: 1
  layer_after_N: 1
  layer_N: 2

  # seq
  share_policy: False
  use_cum_sequence: False
  use_agent_block: False
  use_sequential: True

  # Co-PPO / Sequential
  seq_strategy: 'semi_greedy'
  clip_others: True
  clip_param_tuner: True

  # lora
  train_w0: 0
  train_lora: 1
  train_lora_bias: 0

3s5z_vs_3s6z:
  # training
  n_training_threads: 16

  # algo-ppo
  num_mini_batch: 1
  gain: 0.01
  adv: 'gae_trace'
  gae_lambda: 0.9

  # network
  use_recurrent_policy: True
  stacked_frames: 1
  layer_after_N: 1
  layer_N: 2

  # seq
  share_policy: False
  use_cum_sequence: False
  use_agent_block: False
  use_sequential: True

  # Co-PPO / Sequential
  seq_strategy: 'semi_greedy'
  clip_others: True
  clip_param_tuner: True

  # lora
  train_w0: 0
  train_lora: 1
  train_lora_bias: 0